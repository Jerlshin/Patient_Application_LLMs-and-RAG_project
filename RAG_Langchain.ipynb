{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T08:39:22.929138Z",
     "start_time": "2024-04-16T08:39:16.851540Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "# retrieve all files from the Datapath and convert that into langchain.schema.Document\n",
    "\n",
    "# split into smaller chunks, relavant \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# as each document will be too long for the context window of the LLM, we will split that into k=3\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings # to create the embedding of the database and \n",
    "from langchain.vectorstores.chroma import Chroma # for storing the vectors \n",
    "from langchain.chat_models import ChatOpenAI # LLM \n",
    "\n",
    "# We won't be using OpenAI LLM, Falcon7B instead \n",
    "from langchain.prompts import ChatPromptTemplate # After the RAG, we will send the retrieved and query embed to the LLM\n",
    "from langchain.evaluation import load_evaluator # for evaluating the model \n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings  # Use this instead of openAI \n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# system \n",
    "import os\n",
    "import sys \n",
    "import shutil # for removing the dir \n",
    "\n",
    "# for parsing the query to the retrieved docs \n",
    "import argparse\n",
    "\n",
    "# \n",
    "from dataclasses import dataclass # simplifies the creation of classes to store the data \n",
    "\n",
    "# to load the keys \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# transformers stuff \n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import AutoConfig # to load the model configurations \n",
    "\n",
    "# initialize empty weights for a model \n",
    "from accelerate import init_empty_weights   # for distributed training for using multiple GPUs\n",
    "from accelerate import infer_auto_device_map # to infer the mapping of devices for distrubuted training. # for multiple GPUs \n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "from torch import cuda, bfloat16\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T08:34:40.791421Z",
     "start_time": "2024-04-16T08:34:40.331027Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "# get the config of the model \n",
    "config = AutoConfig.from_pretrained(model) # fetches config, and creates a FalConConfig Object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T08:34:41.327351Z",
     "start_time": "2024-04-16T08:34:41.320969Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FalconConfig {\n",
       "  \"_name_or_path\": \"tiiuae/falcon-7b\",\n",
       "  \"alibi\": false,\n",
       "  \"apply_residual_connection_post_layernorm\": false,\n",
       "  \"architectures\": [\n",
       "    \"FalconForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"auto_map\": {\n",
       "    \"AutoConfig\": \"tiiuae/falcon-7b--configuration_falcon.FalconConfig\",\n",
       "    \"AutoModel\": \"tiiuae/falcon-7b--modeling_falcon.FalconModel\",\n",
       "    \"AutoModelForCausalLM\": \"tiiuae/falcon-7b--modeling_falcon.FalconForCausalLM\",\n",
       "    \"AutoModelForQuestionAnswering\": \"tiiuae/falcon-7b--modeling_falcon.FalconForQuestionAnswering\",\n",
       "    \"AutoModelForSequenceClassification\": \"tiiuae/falcon-7b--modeling_falcon.FalconForSequenceClassification\",\n",
       "    \"AutoModelForTokenClassification\": \"tiiuae/falcon-7b--modeling_falcon.FalconForTokenClassification\"\n",
       "  },\n",
       "  \"bias\": false,\n",
       "  \"bos_token_id\": 11,\n",
       "  \"eos_token_id\": 11,\n",
       "  \"hidden_dropout\": 0.0,\n",
       "  \"hidden_size\": 4544,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"falcon\",\n",
       "  \"multi_query\": true,\n",
       "  \"new_decoder_architecture\": false,\n",
       "  \"num_attention_heads\": 71,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_kv_heads\": 71,\n",
       "  \"parallel_attn\": true,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"torch_dtype\": \"bfloat16\",\n",
       "  \"transformers_version\": \"4.39.3\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 65024\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T08:36:02.631921Z",
     "start_time": "2024-04-16T08:36:02.396197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('transformer.word_embeddings', 0),\n",
       "             ('lm_head', 0),\n",
       "             ('transformer.h.0.self_attention.rotary_emb', 0),\n",
       "             ('transformer.h.0.self_attention.query_key_value', 'cpu'),\n",
       "             ('transformer.h.0.self_attention.dense', 'cpu'),\n",
       "             ('transformer.h.0.self_attention.attention_dropout', 'cpu'),\n",
       "             ('transformer.h.0.mlp', 'cpu'),\n",
       "             ('transformer.h.0.input_layernorm', 'cpu'),\n",
       "             ('transformer.h.1', 'cpu'),\n",
       "             ('transformer.h.2', 'cpu'),\n",
       "             ('transformer.h.3', 'cpu'),\n",
       "             ('transformer.h.4', 'cpu'),\n",
       "             ('transformer.h.5', 'cpu'),\n",
       "             ('transformer.h.6', 'cpu'),\n",
       "             ('transformer.h.7.self_attention', 'cpu'),\n",
       "             ('transformer.h.7.input_layernorm', 'disk'),\n",
       "             ('transformer.h.8', 'disk'),\n",
       "             ('transformer.h.9', 'disk'),\n",
       "             ('transformer.h.10', 'disk'),\n",
       "             ('transformer.h.11', 'disk'),\n",
       "             ('transformer.h.12', 'disk'),\n",
       "             ('transformer.h.13', 'disk'),\n",
       "             ('transformer.h.14', 'disk'),\n",
       "             ('transformer.h.15', 'disk'),\n",
       "             ('transformer.h.16', 'disk'),\n",
       "             ('transformer.h.17', 'disk'),\n",
       "             ('transformer.h.18', 'disk'),\n",
       "             ('transformer.h.19', 'disk'),\n",
       "             ('transformer.h.20', 'disk'),\n",
       "             ('transformer.h.21', 'disk'),\n",
       "             ('transformer.h.22', 'disk'),\n",
       "             ('transformer.h.23', 'disk'),\n",
       "             ('transformer.h.24', 'disk'),\n",
       "             ('transformer.h.25', 'disk'),\n",
       "             ('transformer.h.26', 'disk'),\n",
       "             ('transformer.h.27', 'disk'),\n",
       "             ('transformer.h.28', 'disk'),\n",
       "             ('transformer.h.29', 'disk'),\n",
       "             ('transformer.h.30', 'disk'),\n",
       "             ('transformer.h.31', 'disk'),\n",
       "             ('transformer.ln_f', 'disk'),\n",
       "             ('transformer.h.7.mlp', 'disk')])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creates an empty model based on the configuration loaded \n",
    "with init_empty_weights(): \n",
    "    # LLM\n",
    "    model = AutoModelForCausalLM.from_config(config=config) # model creation - empty \n",
    "\n",
    "# analyzes the empty model's structure to understand the memeory requirements of each layer and param\n",
    "\n",
    "model.tie_weights()  # weight tying between the input and the output embedding \n",
    "\n",
    "device_map = infer_auto_device_map(model)\n",
    "\n",
    "device_map # dictionary mappying "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-16T08:37:22.683154Z",
     "start_time": "2024-04-16T08:37:22.635477Z"
    }
   },
   "outputs": [],
   "source": [
    "# automatically determine the device map from the empty model. maximize all GPU's, then CPU RAM\n",
    "device_map = infer_auto_device_map(\n",
    "    model=model,\n",
    "    no_split_module_classes=[\"OPTDecoderLayer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-16T08:39:59.223279Z"
    },
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-16 14:49:56.046415: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0f3f1007604d03bd3f91794a7b7434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerlshin/env_ai/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#model = \"tiiuae/falcon-40b\"\n",
    "\n",
    "model = \"tiiuae/falcon-7b\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "\n",
    "falcon_pipeline = transformers.pipeline(\n",
    "    \"text-generation\", # task\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,  # by googleBRAIN tea\n",
    "    offload_folder=\"offload\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "   \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"./Dummy_Medical_report/Dummy medicine/\"   # path of the database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.md\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents: list[Document]):\n",
    "    # split the dcoument into chunks \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    \n",
    "    # splitted \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    # original documents and # chunks \n",
    "\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks\")\n",
    "\n",
    "    # random chunk\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma database that uses vector embeddings as the key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = load_evaluator(\"pairwise_embedding_distance\")\n",
    "\n",
    "x = evaluator.evaluate_strings_pairs(prediction=\"apple\", prediction_b=\"orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query for relevant data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "\n",
    "    # remove if database already exists \n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "    \n",
    "    # to generate the chroma vector database from the chunks \n",
    "    db = Chroma.from_documents(\n",
    "        chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH  # this should be the clound or MongoDB\n",
    "    )\n",
    "\n",
    "    db.persist()\n",
    "\n",
    "    # save in .sqlite3\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_data_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to opeimize the output from the RAG \n",
    "\n",
    "PROMTP_TEMPLATE = \"\"\"\n",
    "\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context:\n",
    "\n",
    "{question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer the question based only on the following context in a medical point of view:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Context:\n",
    "{\n",
    "\n",
    " \"name\": \"augmentin 625 duo tablet\",\n",
    "\n",
    " \"substitute0\": \"Penciclav 500 mg/125 mg Tablet\",\n",
    "\n",
    " \"substitute1\": \"Moxikind-CV 625 Tablet\",\n",
    "\n",
    " \"substitute2\": \"Moxiforce-CV 625 Tablet\",\n",
    "\n",
    " \"substitute3\": \"Fightox 625 Tablet\",\n",
    "\n",
    " \"substitute4\": \"Novamox CV 625mg Tablet\",\n",
    "\n",
    " \"sideEffect0\": \"Vomiting\",\n",
    "\n",
    " \"sideEffect1\": \"Nausea\",\n",
    "\n",
    " \"sideEffect2\": \"Diarrhea\",\n",
    "\n",
    " \"sideEffect3\": \"Treatment of Bacterial infections\",\n",
    "\n",
    " \"Habit Forming\": \"ANTI INFECTIVES\",\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Answer the question based on the above context:\n",
    "\n",
    "\n",
    "\n",
    "\"\n",
    "can you give me some details about augmentin 625 duo tablet?\n",
    "\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"query_text\", type=str, help=\"The query text.\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    query_text = args.query_texts\n",
    "\n",
    "    # embedding function should be same as the embedding function we used to create the database\n",
    "    embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "    \"\"\"EMBEDDING\"\"\"\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name = None,\n",
    "        model_kwargs = {\"device\": \"cuda\"}\n",
    "    )\n",
    "\n",
    "    # Do the same \n",
    "    db = Chroma( \n",
    "        persist_directory=CHROMA_PATH,  # data path \n",
    "        embedding_function=embedding_function # function\n",
    "    )\n",
    "\n",
    "    # search the database \n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3) # top 3 results \n",
    "    # List[Tupel[Document, float]]\n",
    "\n",
    "    if len(results) == 0 or results[0][-1] < 0.7:  # threshold \n",
    "        print(f\"unable to find matching results\")\n",
    "        return \n",
    "\n",
    "    # from results, merge the documents  -- convert to single piece of code \n",
    "    context_text = \"\\n\\n--\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    "\n",
    "    # get the template of the prompt \n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMTP_TEMPLATE)\n",
    "\n",
    "    # format of the code \n",
    "    prompt = prompt_template.format(\n",
    "        context=context_text,\n",
    "        question=query_text\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    # LLM, use local model \n",
    "    model = ChatOpenAI()\n",
    "\n",
    "    response_text = model.predict(prompt)\n",
    "\n",
    "\n",
    "    # sourcing the result from the database \n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    "\n",
    "    formatted_response = f\"Response: {response_text}\\n Sources: {sources}\"\n",
    "\n",
    "    print(formatted_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embeddings(text1, text2):\n",
    "    embedding_function = OpenAIEmbeddings()\n",
    "\n",
    "    vector = embedding_function.embed_query(text1)\n",
    "    print(f\"Vector for {text1} : {vector}\")\n",
    "    \n",
    "    evaluator = load_evaluator(\"pairwise_embedding_distance\")\n",
    "    words = (text1, text2)\n",
    "\n",
    "    x = evaluator.evaluate_string_pairs(prediction=words[0], prediction_b=words[1])\n",
    "    print(f\"Comparing ({words[0]}, {words[1]}): {x}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
